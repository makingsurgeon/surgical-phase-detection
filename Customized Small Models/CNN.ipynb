{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf21e67",
   "metadata": {},
   "source": [
    "## Creating Labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27f8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from itertools import count\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6e2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import glob\n",
    "import numpy\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12300d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(224,224),\n",
    "        A.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42edc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdbb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,71):\n",
    "    filename = '/home/zo2151/assignments/Data/Video%i'%(i,)\n",
    "    train_image_paths.append(glob.glob(filename + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67be6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths1 = [item for sublist in train_image_paths for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a347d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths1 = natsort.natsorted(train_image_paths1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024fd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/zo2151/Processed_data.csv\")\n",
    "df1 = df.loc[:,\"Phases\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74497e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0c8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list = pd.DataFrame(\n",
    "    {'Link': train_image_paths1,\n",
    "     'Label': df2,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4543a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list1 = percentile_list.sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c051c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = percentile_list1.loc[:,\"Link\"].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee1c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = percentile_list1.loc[:,\"Label\"].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7559ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths, valid_image_paths = train_image_paths[:int(0.8*len(train_image_paths))], train_image_paths[int(0.8*len(train_image_paths)):]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3df3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, valid_labels = labels[:int(0.8*len(labels))], labels[int(0.8*len(labels)):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbfad05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurgicalDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=False):\n",
    "        super(SurgicalDataset, self).__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a52c1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SurgicalDataset(train_image_paths,train_labels, train_transforms)\n",
    "val_dataset = SurgicalDataset(valid_image_paths,valid_labels, train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532738e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=1024, shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset, batch_size=1024, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5649640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe6644",
   "metadata": {},
   "source": [
    "## The model with CNN and FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0d1b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=44944, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2741eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e5f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/home/zo2151/model1.pt\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8e24e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 169/169 [20:31<00:00,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231.758453130722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 169/169 [19:02<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142.068077325821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 169/169 [19:16<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1156.2235412597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 169/169 [19:06<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146.8516116142273\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # trained 5 epochs, after 1 epoch, connection lost\n",
    "    t = time.time()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm.tqdm(train_loader, total = len(train_loader), leave = True)\n",
    "    for img, label in loop:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = img.to(device), label.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    d = time.time()-t\n",
    "    print(d)\n",
    "    torch.save({\n",
    "            'epoch': 2,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"/home/zo2151/model1.pt\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60c4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305.8759560585022\n",
      "{0: 17, 1: 2, 2: 423, 3: 5243, 4: 39, 5: 268, 6: 9407, 7: 410, 8: 1810, 9: 5444, 10: 208, 11: 9659, 12: 372, 13: 1886}\n",
      "{0: 52, 1: 18, 2: 587, 3: 5470, 4: 213, 5: 288, 6: 11135, 7: 2177, 8: 3001, 9: 5799, 10: 247, 11: 10405, 12: 404, 13: 3216}\n"
     ]
    }
   ],
   "source": [
    "classes = [i for i in range(14)]\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "pr = []\n",
    "pred = []\n",
    "l = []\n",
    "# again no gradients needed\n",
    "t = time.time()\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        l.append(labels)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        m = F.softmax(outputs, dim=1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            pred.append(prediction)\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "        for p in m:\n",
    "            pr.append(p)\n",
    "print(time.time()-t)\n",
    "print(correct_pred)\n",
    "print(total_pred)\n",
    "# print accuracy for each class\n",
    "#for classname, correct_count in correct_pred.items():\n",
    "    #accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    #print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "931aac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l)):\n",
    "    l[i] = l[i].cpu()\n",
    "for i in range(len(l)):\n",
    "    l[i] = l[i].data.numpy()\n",
    "l = [item for sublist in l for item in sublist]\n",
    "for i in range(len(l)):\n",
    "    pred[i] = pred[i].cpu().data.numpy()\n",
    "for i in range(len(l)):\n",
    "    pr[i] = pr[i].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e574b0",
   "metadata": {},
   "source": [
    "## Some Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ae40de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8180972751790198"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(l, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbdf76b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6807584625348103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(l, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b8f3425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.79962193, 0.8035249 , 0.8125    ,\n",
       "       0.96402878, 0.9500101 , 0.98795181, 0.73220065, 0.68229101,\n",
       "       0.96296296, 0.86534671, 0.88151659, 0.61937603])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(l, pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "403f3ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32692308, 0.11111111, 0.72061329, 0.95850091, 0.18309859,\n",
       "       0.93055556, 0.84481365, 0.18833257, 0.60313229, 0.93878255,\n",
       "       0.84210526, 0.9283037 , 0.92079208, 0.58644279])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(l, pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba9a07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(l, pr, multi_class = \"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01954683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842028264837943"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8848b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(l, pr, multi_class = \"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84362cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753112261267696"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9785055",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in net.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in net.buffers()])\n",
    "mem = mem_params + mem_bufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33bb2b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21630504"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
